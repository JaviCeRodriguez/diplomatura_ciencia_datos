{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Argentina programa 4.0 - Módulo 2: Ciencia de Datos\n",
        "\n",
        "---\n",
        "## Ejercícios Semana 6. Regresion Polinomial\n"
      ],
      "metadata": {
        "id": "M5MOMyK2PAFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresión polinomial con datos reales: datos Properati"
      ],
      "metadata": {
        "id": "XaYhhUoU7qh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Volvamos al conjunto de datos Properati, que hemos usado en la semana anterior.\n",
        "\n",
        "Despues de filtrar bien los datos, hicimos un modelo lienal para predecir el valor de las propiedades basados en su superfície:\n",
        "\n",
        "precio [USD]=𝜔0+𝜔1⋅sup. total.\n",
        "\n",
        "* ¿Qué pasa ahora si usamos un modelo polinomial?\n",
        "* ¿Podemos chequear si un modelo más complejo describirá mejor esos datos?\n",
        "* ¿Hasta que orden del polinómio tiene sentido ir?\n",
        "\n"
      ],
      "metadata": {
        "id": "REhD-yzK7wRj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie2zV3qOdUiC"
      },
      "source": [
        "## Otra forma de regularizar: Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAOhuZsjdUiC"
      },
      "source": [
        "Durante la clase, vimos una forma de regularización, conocida por _ridge_ en la que se le agrega a la función error, la suma de los cuadrados de todos los coeficientes, con um peso $\\lambda$/2:\n",
        "$$\n",
        "E_\\text{ridge}(\\boldsymbol{\\omega}; \\lambda) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\n",
        " + \\frac{\\lambda}{2} \\sum_{i=1}^M \\omega_i^2\\;\\;.\n",
        "$$\n",
        "El nuevo término es el término de regularización (o penalización).\n",
        "\n",
        "Como la suma de los cuadrados de todos los coeficientes corresponde al cuadrado de la _norma_ (el módulo) del vector de coeficientes, se dice que esa regularización emplea la _norma_ $l2$.\n",
        "\n",
        "Otra regresión regularizada que se utiliza a menudo es la regresión **LASSO (_least absolute shrinkage and selection operator_ / operador de reducción y selección mínima absoluta)**, que selecciona de forma natural las variables más relevantes y produce modelos más parsimoniosos.\n",
        "\n",
        "En lugar de penalizar la función de error utilizando la suma de los cuadrados de los parámetros del modelo, como en el caso anterior, **LASSO** explota la norma $l1$, que es simplemente la suma de los *valores absolutos* de los parámetros del modelo.\n",
        "\n",
        "En otras palabras, la norma $l1$ de un vector es, simplemente:\n",
        "\n",
        "$$\n",
        "||\\boldsymbol{\\omega}||_1 = \\sum_i |\\omega_i|\\;\\;.\n",
        "$$\n",
        "\n",
        "La función de error modificada es, por lo tanto,\n",
        "$$\n",
        "E_\\text{lasso}(\\boldsymbol{\\omega}; \\lambda) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\n",
        " + \\frac{\\lambda}{2} \\sum_{i=1}^M |\\omega_i|\\;\\;,\n",
        "$$\n",
        "donde nuevamente introducimos el hiperparámetro $\\lambda$ para controlar el nivel de penalización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0KPfXhAdUiC"
      },
      "source": [
        "###Adiós soluciones analíticas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz90zzHNdUiD"
      },
      "source": [
        "La primera consecuencia de esta elección de la penalización es que la función de error ya no puede optimizarse (minimizarse) analíticamente. Es necesario recurrir, entonces, a diferentes algoritmos iterativos.\n",
        "\n",
        "En `sklearn`, hay dos implementaciones:\n",
        "\n",
        "* `linear_model.Lasso` usa *descenso por coordenadas* para encontrar el mínimo de la función de error.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Coordinate_descent.svg/500px-Coordinate_descent.svg.png\" width=500px></img>\n",
        "\n",
        "* `linear_model.LassoLars` utiliza LARS (regresión de ángulo mínimo / _least angle regression_), estrechamente relacionado con _forward stepwise regression_ (es decir, todos los coeficientes comienzan en cero, $\\boldsymbol{\\omega} = 0$, y se incrementan progresivamente). Pueden leer más en la [documentación de scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEjg2nn8dUiD"
      },
      "source": [
        "### Implementación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7haoQJVGdUiD"
      },
      "source": [
        "Importen `Lasso` y `LassoLars` y exploren su documentación y argumentos. ¿Cuáles son los parámetros que están asociados al procedimiento de optimización?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spNPt770dUiD"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso, LassoLars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etlrJXmFdUiE"
      },
      "source": [
        "Creen una función `lasso` tal y como hicimos para la regresión de _ridge_. Elijan una implementación de LASSO. Pueden ver los parámetros de Lasso utilizando `Lasso?`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5jTnK4udUiE"
      },
      "outputs": [],
      "source": [
        "Lasso?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFNCNCQkdUiE"
      },
      "outputs": [],
      "source": [
        "def lasso(m, ll):\n",
        "    return Pipeline([('poly_features', PolynomialFeatures(degree=m)),\n",
        "                     ('regressor', Lasso(alpha=ll/2.0, fit_intercept=False, max_iter=500000))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVrRrgB_dUiE"
      },
      "source": [
        "Utilicen esto para ajustar los datos con *features* polinomiales de grado nueve. Utilicen el mismo parámetro de regularización que el anterior: 0.001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ImCloi_dUiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804215ac-fa2a-45a0-fc8e-301a044a60b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=9)),\n",
              "                ('regressor',\n",
              "                 Lasso(alpha=0.005, fit_intercept=False, max_iter=500000))])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "lasso_pipe = lasso(9, 0.01)\n",
        "lasso_pipe.fit(x_train, t_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywq4jVIJdUiJ"
      },
      "source": [
        "Grafiquen los resultados con la ayuda del código de abajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPNOc2E8dUiK"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(x_train, t_train, 'o', ms=10, mfc='None', label='Entrenamiento')\n",
        "ax.plot(x_, lasso_pipe.predict(x_), 'r-', lw=3, alpha=0.8, label='Curva predicha')\n",
        "ax.plot(x_, ground_truth(x_), 'k-', lw=3, alpha=0.5, label='Modelo real')\n",
        "    #\n",
        "ax.set_title('Grado: {}; $\\lambda$: {:.2e}'.format(lasso_pipe['poly_features'].degree,\n",
        "                                                    lasso_pipe['regressor'].alpha *2), fontsize=16)\n",
        "    #\n",
        "ax.set_ylim(0, 3.9)\n",
        "ax.legend(loc=0, fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tipv4t4PdUiK"
      },
      "source": [
        "A primera vista, parece que ambos regresores producen los mismos resultados. Pero en realidad hay una _gran diferencia_ entre ambos métodos.\n",
        "\n",
        "**Compare los coeficientes encontrados con cada método**. Puede acceder a ellos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observación: para obtener los coeficientes de la rgreción ridge, van a tener que replicar aquí la parte de rige, como está en el notebook de la clase."
      ],
      "metadata": {
        "id": "WHllSVDl2yYa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbVpnlkFdUiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e05dfe-28da-4e85-8250-656c150910b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.84607034,  0.19937739, -0.25641525, -0.2297371 , -0.07640904,\n",
              "         0.08150002,  0.21069503,  0.30738226,  0.37624621,  0.42342855]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "ridge_pipe['regressor'].coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBbnVQx2dUiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9496e892-27eb-47af-d661-a7b0e1117ddf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.08420373,  0.        , -0.        , -1.30247473, -0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  2.34130064])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "lasso_pipe['regressor'].coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ7DjLtqdUiL"
      },
      "source": [
        "**Pregunta**. ¿Ven alguna diferencia?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRuhlKPKdUiL"
      },
      "source": [
        "### Shrinkage recargado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pxTXWtZdUiL"
      },
      "source": [
        "Volvamos a hacer la gráfica de *shrinkage*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUSe9QrGdUiL"
      },
      "outputs": [],
      "source": [
        "# crea los valores de lambda\n",
        "lls = np.logspace(-4, 1, 100)\n",
        "\n",
        "cc_lasso = []\n",
        "\n",
        "# Itera sobre los valors de lambda, ajusta y guarda los valores de los coeficientes\n",
        "for ll in lls:\n",
        "    #print(ll)\n",
        "    lasso_pipe = lasso(degrees[-1], ll)\n",
        "    lasso_pipe.fit(x_train, t_train)\n",
        "    cc_lasso.append(lasso_pipe['regressor'].coef_)\n",
        "\n",
        "cc_lasso = np.array(cc_lasso)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAlRcFJVdUiM"
      },
      "source": [
        "Grafiquen las amplitudes de los coeficientes en función del parámetro de regularización para el caso de Lasso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSyMjVevdUiM"
      },
      "outputs": [],
      "source": [
        "# Valores de los coeficintes versus el parámetro de renormalización.\n",
        "fig = plt.figure(figsize=(8,7))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "for i in range(len(cc_lasso[0])):\n",
        "    ax.semilogx(lls, cc_lasso[:, i], label='$\\omega_{{{}}}$'.format(i), lw=3, alpha=0.6)\n",
        "ax.legend(ncol=3, fontsize=16)\n",
        "ax.set_xlabel('$\\lambda$')\n",
        "ax.set_ylabel('Valor del parámetro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD5tSpqDdUiM"
      },
      "source": [
        "Como pueden ver, a medida que aumentamos el término de regularización, algunos parámetros se van estrictamente a cero. De este modo, la regresión Lasso también funciona como una especie de herramienta de selección automática de modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1MXaZ_ndUiO"
      },
      "source": [
        "# Ejercicio avanzado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GOgB2nAdUiO"
      },
      "source": [
        "Hasta ahora, hemos elegido un polinomio de grado nueve y un valor fijo para el parámetro de regularización. Pero, ¿cómo sabemos que estos valores son los óptimos?\n",
        "\n",
        "Al igual que hicimos con el regresor polinómico no regularizado, podemos utilizar el conjunto de pruebas para evaluar cuál es el valor óptimo del grado *M* y del parámetro de regularización $\\lambda$.\n",
        "\n",
        "* Elija uno de los dos métodos de regularización que hemos discutido anteriormente.\n",
        "* Haga un bucle sobre los valores de grado y lambda y evalúe el modelo utilizando el conjunto de pruebas.\n",
        "* Elegir los mejores valores de $M$ y $\\lambda$ en base a estos resultados.\n",
        "* Repita para el otro regularizador.\n",
        "* Informe de la mejor métrica de rendimiento (MSE) en el conjunto de prueba.\n",
        "* Prepare un gráfico que muestre el MSE de la prueba en función de los valores de los hiperparámetros. Dado que hay dos hiperparámetros, quizás quieras probar con `plt.imshow` o `plt.pcolor`.\n",
        "\n",
        "**Pregunta importante**: ¿piensan que ese valor del error cuadrático medio es representativo de lo que obtendría para datos que aún no se han visto en este proceso? ¿Que podría pasar en ese caso?\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ka01-Nz9eKS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}