{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ejg-75HlvnIZ"
      },
      "source": [
        "# Semana 9\n",
        "\n",
        "## Sobre Umbrales y Metricas en problemas de clasificación, y desbalance de clases\n",
        "\n",
        "* Umbral\n",
        "* P-R Curve\n",
        "* ROC / AUC\n",
        "* Class weights"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vni_zyl7njfH"
      },
      "source": [
        "## Predecir progresión de cancer de mama\n",
        "\n",
        "Vamos a usar un dataset compilado y donado por el Dr. William Wolberg de la Universidad de California at Irvine. Es de 1992 y ya hay mucho trabajo sobre uso de este dataset para desarrollar y evaluar algoritmos de predicción (ver referencias).\n",
        "\n",
        "El dataset tiene:\n",
        "Number of Samples: 569\n",
        "Number of Features: 30 numeric, predictive attributes\n",
        "Number of Classes: 2\n",
        "\n",
        "Referencias:\n",
        "\n",
        " - Wolberg, William. (1992). Breast Cancer Wisconsin (Original). UCI Machine Learning Repository. [Original Wisconsin Breast Cancer Database](https://doi.org/10.24432/C5HP4Z).\n",
        " - O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
        " - William H. Wolberg and O.L. Mangasarian: \"Multisurface method of pattern separation for medical diagnosis applied to breast cytology\", Proceedings of the National Academy of Sciences, U.S.A., Volume 87, December 1990, pp 9193-9196.\n",
        " - O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition via linear programming: Theory and application to medical diagnosis\", in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
        " - K. P. Bennett & O. L. Mangasarian: \"Robust linear programming discrimination of two linearly inseparable sets\", Optimization Methods and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKcnMr2Co97L"
      },
      "outputs": [],
      "source": [
        "# el dataset Wisconsin Breast Cancer (UCI) de Wolberg está en sklearn!\n",
        "from sklearn import datasets\n",
        "\n",
        "# sklearn.datasets nos trae los datos con una estructura particular\n",
        "# sklearn los llama \"bunchs\"\n",
        "# as_frame = True nos devuelve un dataframe pandas embebido en el bunch\n",
        "wbc = datasets.load_breast_cancer(as_frame=True)\n",
        "\n",
        "# y guardamos el dataframe\n",
        "df = wbc.frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwivKziUqJkV"
      },
      "outputs": [],
      "source": [
        "# los bunchs de sklearn son como diccionarios de python\n",
        "# tienen sus propios metodos para acceder a la info\n",
        "print(wbc.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT7QcYg_7y0m"
      },
      "outputs": [],
      "source": [
        "# veamos las etiquetas de las clases (targets)\n",
        "print(wbc.target_names)\n",
        "\n",
        "# cual es la condicion de la etiqueta CERO?\n",
        "print(\"Etiqueta 0 = \", wbc.target_names[0])\n",
        "\n",
        "# y la UNO?\n",
        "print(\"Etiqueta 1 = \", wbc.target_names[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Df0Wdb_Fl7"
      },
      "source": [
        "Si quieren saber mas sobre otros datasets de sklearn (para jugar, aprender, experimentar) vean [sklearn datasets.](https://scikit-learn.org/stable/datasets.html#datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWlx0Fg6GgYW"
      },
      "outputs": [],
      "source": [
        "df.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtep_1C2ATqK"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0cSIime9j8d"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BzJKOaHD7M__"
      },
      "source": [
        "Como convención vamos a usar `x` para guardar los datos de nuestros features (variables), y vamos a mantener en forma separada el diagnóstico (Maligno, Benigno) en la variable `y`.\n",
        "\n",
        "La variable `y`  por lo tanto tiene las etiquetas con las *clases* a las que pertenece cada muestra/paciente. Es nuestro **target** o sea lo que queremos aprender a predecir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLVdBGFP_AJD"
      },
      "outputs": [],
      "source": [
        "# generamos nuestros vectores x (datos) -- copiamos el dataframe entero y le borramos la columna 'target' con las etiquetas\n",
        "x = df.drop(['target'], axis=1)\n",
        "\n",
        "# y nuestro vector y (etiquetas)\n",
        "y = df['target']\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "726C6hKrAW8B"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJlWMoQXAgbq"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOKJK64aAiDh"
      },
      "outputs": [],
      "source": [
        "y.tail()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5wKOPsulBOJZ"
      },
      "source": [
        "## Aprendiendo a clasificar los datos\n",
        "\n",
        "Ahora vamos a intentar aprender a partir de estos datos, y con eso vamos a intentar predecir para cada muestra, la etiqueta o clase a la que pertenece (maligno vs benigno), que va a ser nuestra predicción de pronóstico del cancer.\n",
        "\n",
        "Como en ejemplos anteriores, vamos a usar cross-validation para entrenar el modelo con un conjunto de datos **independiente** y **differente** de los datos con los que vamos a evaluar el modelo (para no hacernos trampa)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e7TD_ONGyf9"
      },
      "outputs": [],
      "source": [
        "# normalizar / estandarizar / re-escalar datos\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#n = (x - np.min(x, axis=0)) / ((np.max(x, axis=0)) - np.min(x, axis=0)).values\n",
        "scaler = MinMaxScaler().fit(x)\n",
        "\n",
        "# n son nuestros datos normalizados!\n",
        "n = scaler.transform(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urwVQUj3B0G5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# notar que usamos n en lugar de x (datos normalizados!)\n",
        "x_train, x_test, y_train, y_test = train_test_split(n, y, test_size=0.15, random_state=666)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXRKBg8XCIFA"
      },
      "outputs": [],
      "source": [
        "# veamos como fue la particion\n",
        "print(\"x train: \", x_train.shape)\n",
        "print(\"x test: \", x_test.shape)\n",
        "print(\"y train: \", y_train.shape)\n",
        "print(\"y test: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttACDyMXCLT6"
      },
      "outputs": [],
      "source": [
        "x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsF225yOEvif"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dnbJa06XCRMo"
      },
      "source": [
        "Ahora entrenemos nuestro modelo logístico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZGLA_IlCVNh"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# y con esto vamos a entrenar\n",
        "lr = LogisticRegression(penalty=\"l2\")\n",
        "lr.fit(x_train,y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JvTiLNdQMLrr"
      },
      "source": [
        "Ya entrenamos el modelo con los datos *train* (`x_train`, `y_train`) ahora vamos a ver cuales son las predicciones para un conjunto de datos que el modelo no vió hasta ahora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEWGVSuJDPoX"
      },
      "outputs": [],
      "source": [
        "# veamos como predice el conjunto de datos de validacion x_test\n",
        "clasificacion = lr.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7TbHw9VDcw4"
      },
      "outputs": [],
      "source": [
        "print(x_test.shape)\n",
        "print(clasificacion.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEP8gFIeFKWn"
      },
      "outputs": [],
      "source": [
        "clasificacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFbTZuKJRUeM"
      },
      "outputs": [],
      "source": [
        "print('Omega_0 = {:.2f}'.format(*lr.intercept_))\n",
        "print('Omega_1 = {:.2f}'.format(lr.coef_[0][0]))\n",
        "print('Omega_2 = {:.2f}'.format(lr.coef_[0][1]))\n",
        "print(\"[...]\")\n",
        "print('Omega_29 = {:.2f}'.format(lr.coef_[0][29]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vZN-8zPc70S"
      },
      "outputs": [],
      "source": [
        "coefficients = lr.coef_[0]\n",
        "import pandas as pd\n",
        "feature_importance = pd.DataFrame({'Feature': x.columns, 'Importance': np.abs(coefficients)})\n",
        "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
        "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7FwFh1kbMnNJ"
      },
      "source": [
        "Y ahora comparemos las etiquetas de la clasificación (predicciones) con las etiquetas *verdaderas* del dataset ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YxVK4JrFPWO"
      },
      "outputs": [],
      "source": [
        "# y ahora comparemos y_test (validacion) vs prediccion\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(f\"Exactitud: {round(accuracy_score(y_test, clasificacion), 3)}      (TP + TN / P + N)\")\n",
        "print(f\"Precisión: {round(precision_score(y_test, clasificacion), 3)}      (TP / TP + FP)\")\n",
        "print(f\"Exhaustividad: {round(recall_score(y_test, clasificacion), 3)}  (TP / TP + FN)\")\n",
        "print(\"\\nMatriz de confusión:\")\n",
        "print(metrics.confusion_matrix(y_test, clasificacion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcUJwdo6RRZ2"
      },
      "outputs": [],
      "source": [
        "# otra manera de evaluar la matrix de confusión\n",
        "confusion = metrics.confusion_matrix(y_test, clasificacion)\n",
        "\n",
        " # verdaderos positivos, True Positive\n",
        "TP = confusion[1,1]\n",
        "TN = confusion[0,0]\n",
        "FP = confusion[0,1]\n",
        "FN = confusion[1,0]\n",
        "\n",
        "print(\"TP:\", TP)\n",
        "print(\"TN:\", TN)\n",
        "print(\"FP:\", FP)\n",
        "print(\"FN:\", FN)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2HLsT7_lRtSa"
      },
      "source": [
        "Ahora calculemos las probabilidades y evaluemos las curvas ROC y la de Precision-Recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YON2neKIR2wa"
      },
      "outputs": [],
      "source": [
        "# las probabilidades (scores) de los datos son estas:\n",
        "probabilidades = lr.predict_proba(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6tAzbQpfRy6k"
      },
      "outputs": [],
      "source": [
        "#@title Plotear Curvas ROC + Precision-Recall\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# nos quedamos con las probabilidades positivas solamente\n",
        "positive_probs = probabilidades[:, 1]\n",
        "\n",
        "# Get ROC curve FPR and TPR from true labels vs score values\n",
        "fpr, tpr, _ = roc_curve(y_test, positive_probs)\n",
        "\n",
        "# Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points\n",
        "roc_auc = roc_auc_score(y_test, positive_probs)\n",
        "\n",
        "# Calculate precision and recall from true labels vs score values\n",
        "precision, recall, _ = precision_recall_curve(y_test, positive_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "plt.fill_between(fpr,tpr, step='post', alpha=0.5, color='orange')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.step(recall, precision, color='orange', where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.5, color='orange')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision Recall Curve')\n",
        "plt.grid(True)\n",
        "\n",
        "left  = 0.125  # the left side of the subplots of the figure\n",
        "right = 0.9    # the right side of the subplots of the figure\n",
        "bottom = 0.1   # the bottom of the subplots of the figure\n",
        "top = 0.9      # the top of the subplots of the figure\n",
        "wspace = 0.5   # the amount of width reserved for blank space between subplots\n",
        "hspace = 0.2   # the amount of height reserved for white space between subplots\n",
        "plt.subplots_adjust(left, bottom, right, top, wspace, hspace)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dMW_cOg3chR4"
      },
      "source": [
        "## Balance de los datos\n",
        "\n",
        "Los datos del dataset Breast Cancer Wisconsin no están balanceados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss0wAVRfcwhp"
      },
      "outputs": [],
      "source": [
        "# veamos cuan balanceado estaban nuestro dataset\n",
        "print(y.value_counts())\n",
        "\n",
        "# cual es el desbalance?\n",
        "print(\"Malignant / Beningo:\", y.value_counts()[0] / y.value_counts()[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6xL-dg2iiFD"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nxo2NMCj86r"
      },
      "outputs": [],
      "source": [
        "# volvemos a entrenar el modelo, ahora con class_weights\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# preprocessing\n",
        "scaler = MinMaxScaler().fit(x)\n",
        "\n",
        "# n son nuestros datos normalizados!\n",
        "n = scaler.transform(x)\n",
        "\n",
        "# split entre train / test\n",
        "#x_train, x_test, y_train, y_test = train_test_split(n, y, test_size = 0.15, random_state=21)\n",
        "x_train, x_test, y_train, y_test = train_test_split(n, y, test_size=0.15, random_state=666)\n",
        "\n",
        "#n_test = scaler.transform(x_test)\n",
        "\n",
        "lr =  LogisticRegression(class_weight='balanced')\n",
        "lr.fit(x_train, y_train)\n",
        "clasificacion = lr.predict(x_test)\n",
        "probabilidades = lr.predict_proba(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpB8x_D-k_uz"
      },
      "outputs": [],
      "source": [
        "# y ahora comparemos y_test (validacion) vs prediccion\n",
        "from sklearn import metrics\n",
        "print(metrics.accuracy_score(y_test, clasificacion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhuOAtFvlRG8"
      },
      "outputs": [],
      "source": [
        "print(metrics.confusion_matrix(y_test, clasificacion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGKRbuCClb-i"
      },
      "outputs": [],
      "source": [
        "coeficientes = lr.coef_[0]\n",
        "import pandas as pd\n",
        "feature_importance = pd.DataFrame({'Feature': x.columns, 'Importance': np.abs(coeficientes)})\n",
        "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
        "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NRzJf9zoPLtX"
      },
      "outputs": [],
      "source": [
        "#@title Plotear Curvas ROC + Precision-Recall\n",
        "\n",
        "# nos quedamos con las probabilidades positivas solamente\n",
        "positive_probs = probabilidades[:, 1]\n",
        "\n",
        "\n",
        "# Get ROC curve FPR and TPR from true labels vs score values\n",
        "#fpr, tpr, _ = roc_curve(y, x)\n",
        "fpr, tpr, _ = roc_curve(y_test, positive_probs)\n",
        "\n",
        "# Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points\n",
        "#roc_auc = auc(fpr, tpr)\n",
        "roc_auc = roc_auc_score(y_test, positive_probs)\n",
        "\n",
        "# Calculate precision and recall from true labels vs score values\n",
        "precision, recall, _ = precision_recall_curve(y_test, positive_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "plt.fill_between(fpr,tpr, step='post', alpha=0.5, color='orange')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.step(recall, precision, color='orange', where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.5, color='orange')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision Recall Curve')\n",
        "plt.grid(True)\n",
        "\n",
        "left  = 0.125  # the left side of the subplots of the figure\n",
        "right = 0.9    # the right side of the subplots of the figure\n",
        "bottom = 0.1   # the bottom of the subplots of the figure\n",
        "top = 0.9      # the top of the subplots of the figure\n",
        "wspace = 0.5   # the amount of width reserved for blank space between subplots\n",
        "hspace = 0.2   # the amount of height reserved for white space between subplots\n",
        "plt.subplots_adjust(left, bottom, right, top, wspace, hspace)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8bH2xqBEOv7r"
      },
      "source": [
        "## Apendice -- Jugar con separacion y desbalance entre clases"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5DgwHmJVx-gh"
      },
      "source": [
        "## Generamos nuestros datos\n",
        "\n",
        "Vamos a generar dos clases con distribución normal (simétrica, gausiana). En ambos casos las clases muestran distribuciones _balanceadas_.\n",
        "\n",
        "Para imaginar situaciones reales, piensen en que las dos clases pueden ser dos grupos de pacientes (enfermos, sanos). O dos grupos de estudiantes (universitarios _vs_ colegio secundario). O dos grupos de atletas entrenados para correr 100 metros (jamaiquinos _vs_ argentinos).\n",
        "\n",
        "En cualquiera de estos ejemplos, contamos con el resultado numérico de algún un test nos va a permitir (con suerte) separar a estas dos clases. Por ejemplo: velocidad en que corren los 100metros (corredores), resultado de medir algun parámetro en sangre en un análisis bioquímico de laboratorio (pacientes), o resultado de la nota en un examen escrito o multiple choice (alumnos).\n",
        "\n",
        "El test nos da un puntaje numérico con valores entre 0 y 10. En el caso de un examen escrito para estudiantes, no nos cuesta mucho imaginarlo. En el caso de tiempo en correr 100m (atletas) o en el caso de algun análisis diagnóstico de laboratorio, imaginemos que los puntajes fueron _normalizados_ o _re-escalados_ para que entren en este rango."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jer-9ljbvf0A"
      },
      "outputs": [],
      "source": [
        "# Setup | Preparación\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkr79AJaAt3v"
      },
      "outputs": [],
      "source": [
        "# vmos a generar datos balanceados (10000 por lado)\n",
        "N1 = 10000\n",
        "media1 = 4\n",
        "sigma1 = 1\n",
        "\n",
        "N2 = 10000\n",
        "media2 = 6\n",
        "sigma2 = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IIK8xO395lQH"
      },
      "outputs": [],
      "source": [
        "#@title Generar Datos y Plotear Dataset\n",
        "#\n",
        "def distribucion_normal(media, sigma, N, bins, label, plt_color, is_plot=True):\n",
        "  # simulamos puntajes con una distribucion a partir de la media, desvio y nro de datos (N)\n",
        "  puntajes = np.sort(norm.rvs(loc=media, scale=sigma, size=N))\n",
        "\n",
        "  # Curva Bell de Referencia\n",
        "  x = np.linspace(-5, 5, N)\n",
        "  # por defecto centrada en 0 y con desvio 1\n",
        "  bell = norm.pdf(x) * N * max(norm.pdf(x))\n",
        "\n",
        "  if is_plot:\n",
        "    plt.hist(puntajes, density=False, color=plt_color, alpha=.5, bins=bins)\n",
        "    plt.plot(x+media, bell, '--', label=label, color=plt_color)\n",
        "\n",
        "  return puntajes, x, bell\n",
        "\n",
        "BINS = 20\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Distribucion de datos etiquetados como Negativos\n",
        "label1 = 'Etiquetas Negativas'\n",
        "puntajes1, x1, bell1 = distribucion_normal(media1, sigma1, N1, BINS, label1, plt_color='red')\n",
        "\n",
        "label2 = 'Etiquetas Positivas'\n",
        "puntajes2, x2, bell2 = distribucion_normal(media2, sigma2, N2, BINS, label2, plt_color='green')\n",
        "\n",
        "# Finalmente generamos nuestros vectores x (datos) + y (etiquetas)\n",
        "y = N1*[0] + N2*[1]\n",
        "\n",
        "x = np.concatenate((puntajes1, puntajes2), axis=None).reshape(-1,1)\n",
        "\n",
        "# Plot\n",
        "plt.title(\"Plots de datos etiquetados Negativos y Positivos\")\n",
        "plt.xlabel('Puntaje')\n",
        "plt.ylabel('Numero de eventos')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "phag_q5GILjJ"
      },
      "source": [
        "El código oculto nos genera datos en nuestros vectores `x`(datos, valores) e `y` (clases, etiquetas). Las etiquetas son: POSITIVO (`1`) o NEGATIVO (`0`). Y los puntajes son los valores numéricos que surjen de algún test. Con esta informacion entrenamos un modelo Logistico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyVeBSfq21o5"
      },
      "outputs": [],
      "source": [
        "# entrenar el modelo\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# con pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# split entre train / test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state=21)\n",
        "\n",
        "pipe = Pipeline([\n",
        "        ('scale', MinMaxScaler()),\n",
        "        ('logistic_regression', LogisticRegression()),\n",
        "        ])\n",
        "\n",
        "pipe.fit(x_train, y_train)\n",
        "clasificacion = pipe.predict(x_test)\n",
        "probabilidades = pipe.predict_proba(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgDnPsvM_QPQ"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(f\"Exactitud: {round(accuracy_score(y_test, clasificacion), 3)}      (TP + TN / P + N)\")\n",
        "print(f\"Precisión: {round(precision_score(y_test, clasificacion), 3)}      (TP / TP + FP)\")\n",
        "print(f\"Exhaustividad: {round(recall_score(y_test, clasificacion), 3)}  (TP / TP + FN)\")\n",
        "print(\"\\nMatriz de confusión:\")\n",
        "print(metrics.confusion_matrix(y_test, clasificacion))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NHPAt4kwLx2P"
      },
      "source": [
        "## Calculamos las probabilidades (scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHDdgmDgLnfQ"
      },
      "outputs": [],
      "source": [
        "# las probabilidades (scores) de los datos son estas:\n",
        "probabilidades = pipe.predict_proba(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lybgVl2jFAA-"
      },
      "outputs": [],
      "source": [
        "#@title Plotear Curvas ROC + Precision-Recall\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# nos quedamos solo con las probabilidades de los eventos positivos\n",
        "positive_probs = probabilidades[:, 1]\n",
        "\n",
        "# Get ROC curve FPR and TPR from true labels vs score values\n",
        "#fpr, tpr, _ = roc_curve(y, x)\n",
        "fpr, tpr, _ = roc_curve(y_test, positive_probs)\n",
        "\n",
        "# Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points\n",
        "#roc_auc = auc(fpr, tpr)\n",
        "roc_auc = roc_auc_score(y_test, positive_probs)\n",
        "\n",
        "# Calculate precision and recall from true labels vs score values\n",
        "precision, recall, _ = precision_recall_curve(y_test, positive_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "plt.fill_between(fpr,tpr, step='post', alpha=0.5, color='orange')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.step(recall, precision, color='orange', where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.5, color='orange')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision Recall Curve')\n",
        "plt.grid(True)\n",
        "\n",
        "left  = 0.125  # the left side of the subplots of the figure\n",
        "right = 0.9    # the right side of the subplots of the figure\n",
        "bottom = 0.1   # the bottom of the subplots of the figure\n",
        "top = 0.9      # the top of the subplots of the figure\n",
        "wspace = 0.5   # the amount of width reserved for blank space between subplots\n",
        "hspace = 0.2   # the amount of height reserved for white space between subplots\n",
        "plt.subplots_adjust(left, bottom, right, top, wspace, hspace)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psNNlAZLLZKP"
      },
      "outputs": [],
      "source": [
        "# generar una prediccion tonta\n",
        "# a todo el conjunto de testeo / validacion le asignamos probabilidad 0\n",
        "# (o 1, da igual)\n",
        "positive_probs = [0.8 for _ in range(len(y_test))]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FGP_1HjgZmbl"
      },
      "source": [
        "# Desbalanceo de datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru4DMDUXZ-VR"
      },
      "outputs": [],
      "source": [
        "# datos desbalanceados\n",
        "N1 = 10000\n",
        "media1 = -1\n",
        "sigma1 = 1\n",
        "\n",
        "N2 = 3000\n",
        "media2 = 1\n",
        "sigma2 = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZHIq5oU4aSpO"
      },
      "outputs": [],
      "source": [
        "#@title Generar Datos y Plotear Dataset\n",
        "#\n",
        "def distribucion_normal(media, sigma, N, bins, label, plt_color, is_plot=True):\n",
        "  # simulamos puntajes con una distribucion a partir de la media, desvio y nro de datos (N)\n",
        "  puntajes = np.sort(norm.rvs(loc=media, scale=sigma, size=N))\n",
        "\n",
        "  # Curva Bell de Referencia\n",
        "  x = np.linspace(-5, 5, N)\n",
        "  # por defecto centrada en 0 y con desvio 1\n",
        "  bell = norm.pdf(x) * N * max(norm.pdf(x))\n",
        "\n",
        "  if is_plot:\n",
        "    plt.hist(puntajes, density=False, color=plt_color, alpha=.5, bins=bins)\n",
        "    plt.plot(x+media, bell, '--', label=label, color=plt_color)\n",
        "\n",
        "  return puntajes, x, bell\n",
        "\n",
        "BINS = 20\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Distribucion de datos etiquetados como Negativos\n",
        "label1 = 'Etiquetas Negativas'\n",
        "puntajes1, x1, bell1 = distribucion_normal(media1, sigma1, N1, BINS, label1, plt_color='red')\n",
        "\n",
        "label2 = 'Etiquetas Positivas'\n",
        "puntajes2, x2, bell2 = distribucion_normal(media2, sigma2, N2, BINS, label2, plt_color='green')\n",
        "\n",
        "# Finalmente generamos nuestros vectores x (datos) + y (etiquetas)\n",
        "y = N1*[0] + N2*[1]\n",
        "\n",
        "x = np.concatenate((puntajes1, puntajes2), axis=None).reshape(-1,1)\n",
        "\n",
        "# Plot\n",
        "plt.title(\"Plots de datos etiquetados Negativos y Positivos\")\n",
        "plt.xlabel('Puntaje')\n",
        "plt.ylabel('Numero de eventos')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSGSVUcrbcNh"
      },
      "outputs": [],
      "source": [
        "# entrenar el modelo\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# con pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# split entre train / test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state=21)\n",
        "\n",
        "pipe = Pipeline([\n",
        "        ('scale', MinMaxScaler()),\n",
        "        ('logistic_regression', LogisticRegression()),\n",
        "        ])\n",
        "\n",
        "pipe.fit(x_train, y_train)\n",
        "clasificacion = pipe.predict(x_test)\n",
        "probabilidades = pipe.predict_proba(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMRr0z3SbcNh"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(f\"Exactitud: {round(accuracy_score(y_test, clasificacion), 3)}      (TP + TN / P + N)\")\n",
        "print(f\"Precisión: {round(precision_score(y_test, clasificacion), 3)}      (TP / TP + FP)\")\n",
        "print(f\"Exhaustividad: {round(recall_score(y_test, clasificacion), 3)}  (TP / TP + FN)\")\n",
        "print(\"\\nMatriz de confusión:\")\n",
        "print(metrics.confusion_matrix(y_test, clasificacion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GauQ18cZ-g2"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xPUeWaDZ-g2"
      },
      "outputs": [],
      "source": [
        "# volvemos a entrenar el modelo, ahora con class_weights\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# con pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# split entre train / test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state=21)\n",
        "\n",
        "pipe = Pipeline([\n",
        "        ('scale', MinMaxScaler()),\n",
        "        ('logistic_regression', LogisticRegression(class_weight='balanced')),\n",
        "        ])\n",
        "\n",
        "pipe.fit(x_train, y_train)\n",
        "clasificacion = pipe.predict(x_test)\n",
        "probabilidades = pipe.predict_proba(x_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGXIe-sVc9an"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(f\"Exactitud: {round(accuracy_score(y_test, clasificacion), 3)}      (TP + TN / P + N)\")\n",
        "print(f\"Precisión: {round(precision_score(y_test, clasificacion), 3)}      (TP / TP + FP)\")\n",
        "print(f\"Exhaustividad: {round(recall_score(y_test, clasificacion), 3)}  (TP / TP + FN)\")\n",
        "print(\"\\nMatriz de confusión:\")\n",
        "print(metrics.confusion_matrix(y_test, clasificacion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaGrqmb3bIZs"
      },
      "outputs": [],
      "source": [
        "#@title Plotear Curvas ROC + Precision-Recall\n",
        "\n",
        "# nos quedamos solo con las probabilidades de los eventos positivos\n",
        "positive_probs = probabilidades[:, 1]\n",
        "\n",
        "# Get ROC curve FPR and TPR from true labels vs score values\n",
        "#fpr, tpr, _ = roc_curve(y, x)\n",
        "fpr, tpr, _ = roc_curve(y_test, positive_probs)\n",
        "\n",
        "# Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points\n",
        "#roc_auc = auc(fpr, tpr)\n",
        "roc_auc = roc_auc_score(y_test, positive_probs)\n",
        "\n",
        "# Calculate precision and recall from true labels vs score values\n",
        "precision, recall, _ = precision_recall_curve(y_test, positive_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "plt.fill_between(fpr,tpr, step='post', alpha=0.5, color='orange')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.step(recall, precision, color='orange', where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.5, color='orange')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision Recall Curve')\n",
        "plt.grid(True)\n",
        "\n",
        "left  = 0.125  # the left side of the subplots of the figure\n",
        "right = 0.9    # the right side of the subplots of the figure\n",
        "bottom = 0.1   # the bottom of the subplots of the figure\n",
        "top = 0.9      # the top of the subplots of the figure\n",
        "wspace = 0.5   # the amount of width reserved for blank space between subplots\n",
        "hspace = 0.2   # the amount of height reserved for white space between subplots\n",
        "plt.subplots_adjust(left, bottom, right, top, wspace, hspace)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwjYB5WWauPL"
      },
      "outputs": [],
      "source": [
        "#@title Plotear Curvas ROC + Precision-Recall\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# nos quedamos solo con las probabilidades de los eventos positivos\n",
        "positive_probs = probabilidades[:, 1]\n",
        "\n",
        "# Get ROC curve FPR and TPR from true labels vs score values\n",
        "#fpr, tpr, _ = roc_curve(y, x)\n",
        "fpr, tpr, _ = roc_curve(y_test, positive_probs)\n",
        "\n",
        "# Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points\n",
        "#roc_auc = auc(fpr, tpr)\n",
        "roc_auc = roc_auc_score(y_test, positive_probs)\n",
        "\n",
        "# Calculate precision and recall from true labels vs score values\n",
        "precision, recall, _ = precision_recall_curve(y_test, positive_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\n",
        "plt.fill_between(fpr,tpr, step='post', alpha=0.5, color='orange')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.step(recall, precision, color='orange', where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.5, color='orange')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision Recall Curve')\n",
        "plt.grid(True)\n",
        "\n",
        "left  = 0.125  # the left side of the subplots of the figure\n",
        "right = 0.9    # the right side of the subplots of the figure\n",
        "bottom = 0.1   # the bottom of the subplots of the figure\n",
        "top = 0.9      # the top of the subplots of the figure\n",
        "wspace = 0.5   # the amount of width reserved for blank space between subplots\n",
        "hspace = 0.2   # the amount of height reserved for white space between subplots\n",
        "plt.subplots_adjust(left, bottom, right, top, wspace, hspace)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
