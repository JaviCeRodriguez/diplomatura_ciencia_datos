{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semana 4 | Boosting\n",
        "\n",
        "Qué es boosting? Boosting es un técnica que se usa en ensambles de modelos para **reforzar** el aprendizaje de modelos de base, que *a priori* consideramos que son malos aprendedores o aprendedores débiles. Esto quiere decir que estos modelos captan algo de la señal informativa en los datos, o en alguna región de los datos, aun cuando no puedan generalizar bien al conjunto de *todos* los datos.\n",
        "\n",
        "Hay muchos métodos de aprendizaje que nos pueden servir como modelos de base. Arboles de decisión son los más comunes, pero no los únicos.\n",
        "\n",
        "En este notebook vamos a usar diferentes métodos de Boosting en tareas de clasificacion (prediccion de etiquetas) y regresión (predicción de valores continuos).\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "  - [SciKit Learn: Ensemble Methods](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)"
      ],
      "metadata": {
        "id": "i27cPik8j1BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Celdas preparatorias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from scipy.stats import norm, multivariate_normal\n",
        "\n",
        "#Ahora las funciones utiles de sklearn para preprocesar datos y armar un pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sklearn.preprocessing as pp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Las estrellas de este notebook\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "    if contour:\n",
        "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
        "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
        "    plt.axis(axes)\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PMAtH6dLkTVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparar (hacer) nuestros datos: make moons\n",
        "\n",
        "Hagamos un dataset sintético con `make_moons` y apliquemosle algo de ruido, para que no sea tan fácil. Esto ya lo hicimos en clases pasadas."
      ],
      "metadata": {
        "id": "rKf3ZghEkTBq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5y9ZiszjsF4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "X, t = make_moons(n_samples=500, noise=0.15, random_state=42)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=t+5, cmap='rainbow')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, t, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaBoost Classifier\n",
        "\n",
        "### SAMME vs SAMME.R\n",
        "\n",
        "Discrete SAMME AdaBoost adapts based on errors in predicted class labels whereas real SAMME.R uses the predicted class probabilities."
      ],
      "metadata": {
        "id": "LX0AvbnKlCTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ne = 50 # number of estimators\n",
        "# en boosting = numero de etapas de boosting\n",
        "# en random forest / extra trees = numero de arboles en el bosque\n",
        "\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth = 1), # esto es un stump = weak learner\n",
        "    n_estimators = ne,\n",
        "    algorithm = \"SAMME\", learning_rate = 0.5, random_state = 42\n",
        "    #algorithm = \"SAMME.R\", learning_rate = 0.5, random_state = 42\n",
        ")\n",
        "\n",
        "ada_clf.fit(X_train,y_train)\n",
        "plot_decision_boundary(ada_clf, X, t)"
      ],
      "metadata": {
        "id": "o92ukEFjlEXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(ada_clf, X, t, cv=5, scoring='accuracy')"
      ],
      "metadata": {
        "id": "S5UH2OBuobvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores.mean())\n",
        "print(scores.std())"
      ],
      "metadata": {
        "id": "RUDhxACcoxsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparacion con ExtraTrees"
      ],
      "metadata": {
        "id": "oPA5OnzupIpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "etc_clf = ExtraTreesClassifier(n_estimators = ne, max_depth = 1, max_features=1)\n",
        "etc_clf.fit(X_train, y_train)\n",
        "plot_decision_boundary(etc_clf, X, t)"
      ],
      "metadata": {
        "id": "SCf9CVs5pMoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(etc_clf, X, t, cv=5, scoring='accuracy')"
      ],
      "metadata": {
        "id": "DmsNsMEwp3kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores.mean())\n",
        "print(scores.std())"
      ],
      "metadata": {
        "id": "GzPyjDezp3kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparacion con RandomForests\n",
        "\n",
        "Aprendizaje paralelo: boostrap + aggregation (bagging)"
      ],
      "metadata": {
        "id": "i28YFaWvpnAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_estimators = ne, max_depth = 7, random_state = 42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "plot_decision_boundary(rf_clf, X, t)"
      ],
      "metadata": {
        "id": "90YoqdSVpyPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(rf_clf, X, t, cv=5, scoring='accuracy')"
      ],
      "metadata": {
        "id": "PeZU3m_OqXKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores.mean())\n",
        "print(scores.std())"
      ],
      "metadata": {
        "id": "dRKVQi9dqXKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparacion con Gradient Boosting"
      ],
      "metadata": {
        "id": "B1FPAW-eeGo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators = ne, learning_rate = 1.0,\n",
        "                                    max_depth = 1, random_state = 42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "plot_decision_boundary(gb_clf, X, t)\n"
      ],
      "metadata": {
        "id": "lEIG_4P_dSIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(gb_clf, X, t, cv=5, scoring='accuracy')\n"
      ],
      "metadata": {
        "id": "epdQKmXoeBNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores.mean())\n",
        "print(scores.std())"
      ],
      "metadata": {
        "id": "E4FzvzwweMVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparacion con XGBoost"
      ],
      "metadata": {
        "id": "Togq69yVdTI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgb_clf = xgb.XGBClassifier(objective ='binary:logistic', n_estimators = ne)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "plot_decision_boundary(xgb_clf, X, t)"
      ],
      "metadata": {
        "id": "VrkmKFaJdSUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(xgb_clf, X, t, cv=5, scoring='accuracy')\n"
      ],
      "metadata": {
        "id": "g-H2E7TArnZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores.mean())\n",
        "print(scores.std())"
      ],
      "metadata": {
        "id": "VzREthVIrnZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pasemos a tareas de Regresión"
      ],
      "metadata": {
        "id": "JF2CaWng0-h-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis con datos más complejos (California Housing Dataset 1990)\n",
        "\n",
        "Vamos a analizar ahora el dataset de casas de California, dataset clásico en ciencia de datos. Como recordatorio, acá van las variables de este dataset:\n",
        "\n",
        "### About this file\n",
        " 1. `longitude`: A measure of how far west a house is; a higher value is farther west\n",
        " 2. `latitude`: A measure of how far north a house is; a higher value is farther north\n",
        " 3. `housingMedianAge`: Median age of a house within a block; a lower number is a newer building\n",
        " 4. `totalRooms`: Total number of rooms within a block\n",
        " 5. `totalBedrooms`: Total number of bedrooms within a block\n",
        " 6. `population`: Total number of people residing within a block\n",
        " 7. `households`: Total number of households, a group of people residing within a home unit, for a block\n",
        " 8. `medianIncome`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
        " 9. `medianHouseValue`: Median house value for households within a block (measured in US Dollars)\n",
        " 10. `oceanProximity`: Location of the house with respect to ocean/sea\n",
        "\n",
        "Referencia: [California Housing Data Set Description](https://developers.google.com/machine-learning/crash-course/california-housing-data-description)"
      ],
      "metadata": {
        "id": "iAHELLCzMdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carguemos los datos"
      ],
      "metadata": {
        "id": "qG0snqcYI0nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conectar Drive a Colab\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montar drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "# Cambiar directorio\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/Data/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFwqznn4GawP",
        "outputId": "bfd6b42c-19bd-43da-b43a-938c6653971f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "california_housing = pd.read_csv('housing.zip', compression = 'zip')"
      ],
      "metadata": {
        "id": "vXFX6WcuGw5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploremos los datos"
      ],
      "metadata": {
        "id": "eGZ-cAI2I4u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing.head(8)"
      ],
      "metadata": {
        "id": "FlEEkktFI8LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing.describe()"
      ],
      "metadata": {
        "id": "HdgSxsffIe4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing.count()"
      ],
      "metadata": {
        "id": "cun7QaqQIjN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Curemos los datos\n",
        "\n",
        "La idea de esta parte del ejercicio es insistir en el concepto importante de curar o limpiar los datos. Identificar valores faltantes, outliers, variables redundantes entre sí, etc."
      ],
      "metadata": {
        "id": "9OmGgt7lI-hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valores faltantes"
      ],
      "metadata": {
        "id": "lcI3PuA6JeKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay valores faltantes en la variable `total_bedrooms`. Podemos hacer dos cosas: 1) reemplazar los registros con valores faltantes (tirarlos = drop), o 2) reemplazar el valor faltante por la media (promedio). Acá yo elegí esto último."
      ],
      "metadata": {
        "id": "pwARGH8gIpQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing['total_bedrooms'].fillna(california_housing['total_bedrooms'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "z2U8yEhwJCYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing.count()\n"
      ],
      "metadata": {
        "id": "Nu3bI12rJK8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Codifiquemos variables categóricas"
      ],
      "metadata": {
        "id": "udkLRgg7JVEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A ver que hay en \"ocean_proximity\"\n",
        "california_housing['ocean_proximity'].value_counts()"
      ],
      "metadata": {
        "id": "dQHakT2jJP1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformemos estas variables categóricas en numéricas:"
      ],
      "metadata": {
        "id": "93OMPpJ5JYtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nuestras categorias de proximidad ahora son numeros\n",
        "def transformar_ocean_proximity(proximity):\n",
        "    if proximity == '<1H mar':\n",
        "        return 0\n",
        "    elif proximity == 'Tierra adentro':\n",
        "        return 1\n",
        "    elif proximity == 'Cerca mar':\n",
        "        return 2\n",
        "    elif proximity == 'Cerca bahía':\n",
        "        return 3\n",
        "    elif proximity == 'Isla':\n",
        "        return 4\n",
        "california_housing['ocean_proximity'] = california_housing['ocean_proximity'].apply(transformar_ocean_proximity)"
      ],
      "metadata": {
        "id": "wfFqGwSNJftz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Y a ver ahora?\n",
        "california_housing['ocean_proximity'].value_counts()"
      ],
      "metadata": {
        "id": "G2nBWf8hPlRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# miremos los datos\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "corr = california_housing.corr()\n",
        "\n",
        "# numpy zeros_like: return an array of zeros with the same shape and type as a given array.\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "\n",
        "# y esto es para obtener solamente un triangulo de la matriz simétrica\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "sns.heatmap(california_housing.corr(), linewidths=.5, annot = True, mask = mask, cmap = 'PiYG')"
      ],
      "metadata": {
        "id": "jrxut-fXKKeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removamos variables redundantes"
      ],
      "metadata": {
        "id": "laFK_YKQJmRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hay una obvia correlación (alta) entre viviendas (households) y poblacion (population)\n",
        "# vamos a remover las viviendas\n",
        "california_housing.drop('households', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "4dZcZX6KQ0z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y hay otra correlación (alta) entre número de ambientes por cuadra y número de habitaciones (para dormir) por cuadra\n",
        "\n",
        "# asi que las vamos a reemplazar por dos nuevas variables:\n",
        "# promedio de ambientes por población en la cuadra (block)\n",
        "# y promedio de habitaciones por población en la cuadra\n",
        "california_housing['average_rooms']    = california_housing['total_rooms'] / california_housing['population']\n",
        "california_housing['average_bedrooms'] = california_housing['total_bedrooms'] / california_housing['population']\n",
        "\n",
        "# asi que vamos a descartar estas variables\n",
        "california_housing.drop('total_rooms', axis = 1, inplace = True)\n",
        "california_housing.drop('total_bedrooms', axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "L9fTPO67RLuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# miremos los nuevos datos\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "corr = california_housing.corr()\n",
        "\n",
        "# numpy zeros_like: return an array of zeros with the same shape and type as a given array.\n",
        "mask = np.zeros_like(corr, dtype=bool)\n",
        "\n",
        "# y esto es para obtener solamente un triangulo de la matriz simétrica\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "sns.heatmap(california_housing.corr(), linewidths=.5, annot = True, mask = mask, cmap = 'PiYG')"
      ],
      "metadata": {
        "id": "c9txi35DSGxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos los histogramas | las distribuciones de las diferentes variables\n",
        "california_housing.hist(bins=70, figsize=(15,15))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n4NQfszgTEqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Desvio : outliers"
      ],
      "metadata": {
        "id": "2o2nSBSh1LlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hma = california_housing[ california_housing['housing_median_age'] == 52 ]\n",
        "hma.count()"
      ],
      "metadata": {
        "id": "WrL6-0lHW7Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing = california_housing.loc[ california_housing['housing_median_age'] <52,:]"
      ],
      "metadata": {
        "id": "-DBsUovxjNk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "california_housing = california_housing.loc[ california_housing['median_house_value']<500001,:]"
      ],
      "metadata": {
        "id": "fpJIB5MATtbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhv = california_housing[ california_housing['median_house_value']>50000]\n",
        "mhv.count()"
      ],
      "metadata": {
        "id": "RY8vxtOsiK53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usemos los datos\n",
        "\n",
        "Carguemos nuestras variables (features: X) y nuestro valor target que queremos predecir (el valor de las casas: y)."
      ],
      "metadata": {
        "id": "WDiEUHasMwT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = california_housing[['longitude', 'latitude', 'housing_median_age',\n",
        "                        'population', 'median_income', 'ocean_proximity',\n",
        "                        'average_rooms', 'average_bedrooms']]\n",
        "\n",
        "y = california_housing['median_house_value']"
      ],
      "metadata": {
        "id": "oTIdyx2-X-h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partimos el dataset en train + test"
      ],
      "metadata": {
        "id": "dOZpHY5YNARj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "Hw5WyQruIDj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Primero regresion lineal, como comparación"
      ],
      "metadata": {
        "id": "a1nFwn8XbRUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "htqO-wMvYPhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "2prkQXFSYgws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# R2 score | MAE | MSE\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('El R cuadrado de la regresión lineal es:', r2)\n",
        "print('El error absoluto medio es:', mae)\n",
        "print('El error cuadrado medio es:', mse)"
      ],
      "metadata": {
        "id": "47S2eCFfYj-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos graficamente las primeras n predicciones\n",
        "n = 200\n",
        "grp = pd.DataFrame({'prediccion':y_pred,'real':y_test})\n",
        "grp = grp.reset_index()\n",
        "grp = grp.drop(['index'],axis=1)\n",
        "plt.style.use('ggplot')\n",
        "plt.figure(figsize=(9,4.5))\n",
        "plt.plot(grp[:n],linewidth=2)\n",
        "plt.xlabel('Eventos', fontsize=20)\n",
        "plt.ylabel('Median House Value', fontsize=20)\n",
        "plt.legend(['Real','Predicción'],prop={'size': 20})\n",
        "sns.jointplot(x=\"real\", y=\"prediccion\", data=grp, kind=\"reg\")"
      ],
      "metadata": {
        "id": "4pqciIEsYt9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost\n",
        "\n",
        "Parámetros del algoritmo:\n",
        " - objective:\n",
        "   - reg:squarederror: regression with squared loss.\n",
        "   - reg:squaredlogerror: regression with squared log loss\n",
        "   - reg:logistic: logistic regression.\n",
        "   - binary:logistic: logistic regression for binary classification, output probability\n",
        "   - muchos más ...\n",
        " - eta (alias learning_rate): shrinks the feature weights to make the boosting process more conservative.\n",
        " - max_depth: maxima profundida de los arboles\n",
        " - alpha: L1 regularization term on weights. Increasing this value will make model more conservative.\n",
        " - lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
        " - n_estimators: number of boosting stages\n",
        " - booster: Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions.\n",
        "\n",
        "Referencia: [XGBoost Documentation.](https://xgboost.readthedocs.io/en/stable/parameter.html)"
      ],
      "metadata": {
        "id": "XE7rtAZ-b1zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "ne = 300 # number of estimators\n",
        "xr = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 1, eta = 0.3, learning_rate = 0.1, max_depth = 5, alpha = 10, n_estimators = ne)"
      ],
      "metadata": {
        "id": "aRYV6Bheb1Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xr.fit(X_train,y_train)\n",
        "\n",
        "y_pred2 = xr.predict(X_test)"
      ],
      "metadata": {
        "id": "CKk_djmrb-ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos graficamente las primeras n predicciones\n",
        "n = 200\n",
        "grp = pd.DataFrame({'prediccion':y_pred2,'real':y_test})\n",
        "grp = grp.reset_index()\n",
        "grp = grp.drop(['index'],axis=1)\n",
        "plt.style.use('ggplot')\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(grp[:n],linewidth=2)\n",
        "plt.xlabel('Eventos', fontsize=20)\n",
        "plt.ylabel('Median House Value', fontsize=20)\n",
        "plt.legend(['Real','Predicción'],prop={'size': 20})\n",
        "sns.jointplot(x=\"real\", y=\"prediccion\", data=grp, kind=\"reg\")"
      ],
      "metadata": {
        "id": "q9pVOXrbcwAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# R2 score | MAE | MSE\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "r2_xgb = r2_score(y_test,y_pred2)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred2)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred2)\n",
        "print('El R cuadrado de la regresión lineal es:', r2_xgb)\n",
        "print('El error absoluto medio es:', mae_xgb)\n",
        "print('El error cuadrado medio es:', mse_xgb)"
      ],
      "metadata": {
        "id": "KpKQl_LodCox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.plot_importance(xr)\n",
        "plt.rcParams['figure.figsize'] = [5, 5]\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NJRfGGqddMTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Doing cross validation to see the accuracy of the XGboost model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "kfold = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
        "resultados = cross_val_score(xr, X, y, cv = kfold)\n",
        "print(\"Exactitud: %.2f%% (%.2f%%)\" % (resultados.mean() * 100, resultados.std() * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psxG-yH4dj9N",
        "outputId": "48553421-a3c5-43bb-e7b5-316b875e796b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exactitud: 81.59% (1.48%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comparando scores de las dos técnicas (linear regression vs XGBoost)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "# MAE = Mean Absolute Error\n",
        "# RMS = Root Mean Square\n",
        "mae1 = mean_absolute_error(y_test, y_pred)\n",
        "rms1 = sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae2 =mean_absolute_error(y_test, y_pred2)\n",
        "rms2 = sqrt(mean_squared_error(y_test, y_pred2))\n",
        "\n",
        "print('Stadisticas para la regresion lineal: \\n','MSE:', rms1, '\\n R2:', r2,' \\n MAE:', mae1 )\n",
        "print('Stadisticas para xgboost: \\n','MSE:', rms2, '\\n R2:', r2_xgb,' \\n MAE:', mae2 )"
      ],
      "metadata": {
        "id": "08cOaXo1ejnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios\n",
        "\n",
        "## Diamonds dataset\n",
        "\n",
        "El dataset de diamantes tiene más de 50k observaciones de distintos diamantes, con datos sobre:\n",
        "```\n",
        "price price in US dollars ($326--$18,823)\n",
        "\n",
        "carat weight of the diamond (0.2--5.01)\n",
        "\n",
        "cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
        "\n",
        "color diamond colour, from J (worst) to D (best)\n",
        "\n",
        "clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
        "\n",
        "x length in mm (0--10.74)\n",
        "\n",
        "y width in mm (0--58.9)\n",
        "\n",
        "z depth in mm (0--31.8)\n",
        "\n",
        "depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
        "\n",
        "table width of top of diamond relative to widest point (43--95)\n",
        "```\n",
        "\n",
        "Tarea: entrenar varios regresores por distintas técnicas (incluyendo las de boosting) para predecir el valor en dólares de un diamante en base a estos datos.\n",
        "\n",
        " - Cuál es el regresor que mejor performance tiene? (qué algoritmo es?)\n",
        " - Cuáles son los mejores hiperparámetros para el modelo?\n",
        " - Cuál es la performance?\n",
        "\n",
        "Dataset: [Diamonds data at Kaggle, from the Loose Diamonds Search Engine.](https://www.kaggle.com/datasets/shivam2503/diamonds)\n"
      ],
      "metadata": {
        "id": "YbP3xNf0fG9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cLIwXWzP3QZr"
      }
    }
  ]
}