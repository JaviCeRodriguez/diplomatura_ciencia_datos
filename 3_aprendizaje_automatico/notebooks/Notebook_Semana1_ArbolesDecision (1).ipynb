{"cells":[{"cell_type":"markdown","id":"christian-joint","metadata":{"id":"christian-joint"},"source":["# Árboles de Decisión"]},{"cell_type":"markdown","id":"0f7dec03","metadata":{"id":"0f7dec03"},"source":["El algoritmo de árboles de decisión se cuentan entre los más transparentes. Si bien en por sí solos no son los algoritmos más poderosos, a partir de ellos pueden generarse algunos de los algoritmos más poderosos de aprendizaje automático, que veremos más adelante en el curso.\n","\n","En este notebook, tomamos el primer contacto con estos modelos y mostramos algunas de sus características y limitaciones."]},{"cell_type":"markdown","id":"e98ccc1b","metadata":{"id":"e98ccc1b"},"source":["## Conceptos básicos"]},{"cell_type":"markdown","id":"6419d5dc","metadata":{"id":"6419d5dc"},"source":["El algoritmo de **Arboles de Decisión** consiste, en gran parte, en dividir el espacio de datos a partir de decisiones binarias (con respuesta sí / no) sobre alguna de las características, como se muestra en la figura.\n","\n","En cada región identificada, la predicción del modelo es muy sencilla (la moda de la distribución de clases o el promedio de los valores target, según si se trata de clasificación o regresión). Toda la sofisticación está en cómo encontrar la separación que mejor rendimiento tiene en términos de las predicciones que hará el modelo a partir de ella."]},{"cell_type":"markdown","id":"c83ef17f","metadata":{"id":"c83ef17f"},"source":["<img width=600 src=\"https://raw.githubusercontent.com/IAI-UNSAM/datasets/master/images/decision_regions.png\">\n","\n","\n","<!-- ![DT_decision_regions][def]\n","\n","[def]: https://raw.githubusercontent.com/IAI-UNSAM/datasets/master/images/decision_regions.png -->"]},{"cell_type":"markdown","id":"7ba81865","metadata":{"id":"7ba81865"},"source":["Se llaman **árboles** de decisión porque la partición del espacio de las características se hace a partir de una sucesión de preguntas binarias, que pueden expresarse como un árbol.\n","\n","<img width=500 src=\"https://raw.githubusercontent.com/IAI-UNSAM/datasets/master/images/decision_regions.png\">\n","<img width=500 src=\"https://raw.githubusercontent.com/IAI-UNSAM/datasets/master/images/arbol.png\">"]},{"cell_type":"markdown","id":"51204ae5","metadata":{"id":"51204ae5"},"source":["\n","Vamos a ver un ejemplo de clasificación para ir profundizando en el funcionamiento de este algoritmo. Aclaramos que puede usarse igualmente bien para **clasificaciones binarias** o **multi-clase**, y que además los árboles de decisión se pueden implementar para **problemas de regresión**."]},{"cell_type":"markdown","id":"cbb8eb23","metadata":{"id":"cbb8eb23"},"source":["**Ventajas**\n","\n","* Fácil de entender e interpretar: Los árboles se pueden visualizar. White box.\n","* Requiere poca o nula preparación de los datos.\n","* Predicciones muy rápidas.\n","\n","**Desventajas**\n","* Árboles demasiado complejos que no generalizan bien los datos.\n","* Inestables frente a pequeñas variaciones en los datos\n","* No son buenos para la extrapolación.\n","* Árbol óptimo es un problema NP-completo.\n","* Árboles sesgados si algunas clases dominan. Se recomienda equilibrar el conjunto de datos antes de ajustarlo con el árbol de decisión."]},{"cell_type":"markdown","id":"organized-direction","metadata":{"id":"organized-direction"},"source":["## Ejemplo de clasificación"]},{"cell_type":"code","execution_count":null,"id":"native-shower","metadata":{"id":"native-shower"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","\n","plt.style.use('ggplot')"]},{"cell_type":"markdown","id":"negative-equivalent","metadata":{"id":"negative-equivalent"},"source":["Utilizaremos el conjunto de datos Iris con sólo dos características, y vamos a entrenar un árbol de decisión para clasificar los datos según la especie de la flor."]},{"cell_type":"code","execution_count":null,"id":"empty-driving","metadata":{"id":"empty-driving"},"outputs":[],"source":["from sklearn import datasets\n","\n","iris=datasets.load_iris()\n","\n","# Nos quedamos con los dos primeros features\n","datos = iris.data[:, 2:]\n","etiquetas = iris.target\n","\n","# Sacamos los nombres\n","# labelx, labely = iris.feature_names[2:]\n","labelx, labely = ['Largo del pétalo (cm)', 'Ancho del pétalo (cm)']\n"]},{"cell_type":"code","execution_count":null,"id":"0e839b36","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":475},"executionInfo":{"elapsed":1038,"status":"ok","timestamp":1691445273745,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"0e839b36","outputId":"c1c630d6-0e6f-436c-a17d-139f36733c7e"},"outputs":[],"source":["# Graficamos\n","fig = plt.figure()#figsize=(12, 8))\n","ax = fig.add_subplot(111)\n","sct = ax.scatter(datos[:,0], datos[:, 1], c=etiquetas, edgecolors='k', s=9**2,\n","                  cmap=plt.cm.rainbow)\n","\n","legend1 = ax.legend(sct.legend_elements()[0], iris.target_names,\n","                    loc=\"upper left\", title=\"Clases\")\n","ax.add_artist(legend1)\n","\n","plt.xlabel(labelx, fontsize=16)\n","plt.ylabel(labely, fontsize=16)\n"]},{"cell_type":"markdown","id":"mysterious-attack","metadata":{"id":"mysterious-attack"},"source":["Ahora, vamos a importar un `DecisionTreeClassifier` de `sklearn` (esto implementa una versión optimizada del algoritmo CART, incluyendo algunas características adicionales, pero restringido a sólo variables numéricas)."]},{"cell_type":"code","execution_count":null,"id":"prerequisite-semiconductor","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":706,"status":"ok","timestamp":1691445383431,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"prerequisite-semiconductor","outputId":"d0354f44-ff5b-4b73-af8e-3612a3e6b840"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n","tree_clf.fit(datos, etiquetas)"]},{"cell_type":"markdown","id":"expanded-stake","metadata":{"id":"expanded-stake"},"source":["Podemos visualizar el árbol de decisión resultante utilizando el método `plot_tree` de la biblioteca `sklearn.tree`."]},{"cell_type":"code","execution_count":null,"id":"abstract-jerusalem","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":653},"executionInfo":{"elapsed":1091,"status":"ok","timestamp":1691445405675,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"abstract-jerusalem","outputId":"e285e07c-1554-4de9-b982-7145931a88c6"},"outputs":[],"source":["from sklearn.tree import plot_tree\n","\n","plt.figure(figsize=(10, 8))\n","\n","plot_tree(tree_clf,\n","        #   feature_names=iris.feature_names[2:],\n","          feature_names=[labelx, labely],\n","          class_names=iris.target_names,\n","          rounded=True,\n","          filled=True)\n","plt.show()"]},{"cell_type":"markdown","id":"1d5bf7f1","metadata":{"id":"1d5bf7f1"},"source":["Veamos un gráfico al lado del otro e intentemos entender qué está pasando y qué significa cada valor en el gráfico del árbol."]},{"cell_type":"code","execution_count":null,"id":"ed1e9dc0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":1323,"status":"ok","timestamp":1691445445856,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"ed1e9dc0","outputId":"d01dc1fd-6f36-47c2-df4c-8fa272db9612"},"outputs":[],"source":["# Graficamos\n","fig = plt.figure(figsize=(12, 4))\n","\n","ax = fig.add_subplot(1,2,1)\n","ax2 = fig.add_subplot(1,2,2)\n","fig.subplots_adjust(wspace=0)\n","\n","sct = ax.scatter(datos[:,0], datos[:, 1], c=etiquetas, edgecolors='k', s=9**2,\n","                  cmap=plt.cm.rainbow)\n","\n","legend1 = ax.legend(sct.legend_elements()[0], iris.target_names,\n","                    loc=\"upper left\", title=\"Clases\")\n","ax.add_artist(legend1)\n","\n","ax.set_xlabel(labelx, fontsize=16)\n","ax.set_ylabel(labely, fontsize=16)\n","\n","trplot = plot_tree(tree_clf,\n","          feature_names=[labelx, labely],\n","          class_names=iris.target_names,\n","          rounded=True,\n","          filled=True,\n","          ax=ax2)"]},{"cell_type":"markdown","id":"retired-recall","metadata":{"id":"retired-recall"},"source":["Como es de esperar, una vez que el árbol está entrenado (¡todavía no dijimos cómo se hace!) podemos usar los métodos típicos `predict` y `predict_proba` para predecir la clase de un dato nuevo, no observado.\n","\n","El método `predict_proba` devuelve un número que es la fracción de puntos correctamente clasificados del conjunto de entrenamiento en la hoja correspondiente:\n","$$p_k = \\frac{N_k}{ N_{total}}\\;\\;,$$\n","\n","con $$k = \\{\\text{setosa}, \\text{versicolor}, \\text{virginica}\\}$$"]},{"cell_type":"code","execution_count":null,"id":"egyptian-welcome","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1691446644040,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"egyptian-welcome","outputId":"b43a5484-1d40-4d01-a76d-7792c9ddb1cd"},"outputs":[],"source":["# Ejemplo\n","data_points = [\n","    [4, 1.5],\n","]\n","\n","y = tree_clf.predict(data_points)\n","print('Predicción: {} ({})'.format(*y, *iris.target_names[y]))\n","print('Probabilidades', *tree_clf.predict_proba(data_points).round(4))"]},{"cell_type":"markdown","id":"unnecessary-playing","metadata":{"id":"unnecessary-playing"},"source":["Vemos que, de hecho, los números corresponden a las fracciones observadas en la hoja correspondiente del árbol."]},{"cell_type":"markdown","id":"18a07562","metadata":{"id":"18a07562"},"source":["### *Algoritmo CART"]},{"cell_type":"markdown","id":"08bd167a","metadata":{"id":"08bd167a"},"source":["Ahora que entendimos la estructura del árbol, nos concentramos en estudiar cómo se crea esa estructura. El algoritmo más común se llama CART (Classification and Regresión Trees). En cada bifurcación CART elige el *feature* y el valor que produce la máxima reducción de alguna función de costo; en el caso de la clasificación, se trata de un medición basada en alguna de las  métricas que se definen a continuación. Todas ellas disminuyen cuando los nodos terminales de los árboles (las hojas) tienen menor mezcla de clases.\n","\n","1. **Índice de Gini**. En cada nodo, podemos calcular este índice, a partir de la fracción de muestras de cada clase, $p_k$, con $k = \\{\\text{setosa}, \\text{versicolor}, \\text{virginica}\\}$. Para calcular el índice Gini, se suma $p_k (1 - p_k)$ para todas las clases:\n","\n","$$\n","\\sum_{k=1}^K p_k (1 - p_k)\\;\\;.\n","$$\n","\n","Por ejemplo, calculemos el índice de Gini para el nodo raíz, que tiene 50 mustras de cada clase. Es decir, $p_k = 50/150 = 1/3$ para todas las clases. Entonces, tenemos $1/3 \\cdot(1 - 1/3) = 2/9$, para cada clase. El total para ese nodo, será $6/9 \\sim 0.667$, que es lo que figura en el gráfico.\n","\n","Si queremos calcular el índice de Gini después del primer *split*, entonces, tenemos que calcular el índice para cada nuevo nodo y sumarlos, ponderados por la cantidad de muestras que tiene cada uno.\n","\n","En el nodo de la izquierda, tenemos solo muestra de Setosa. En ese caso el índice de Gini es cero. En el nodo de la derecha, tenemos 50 de Versicolor y 50 de Virginica: $50/100 \\cdot (1 - 50/100) = 1/4$ para cada clase. Por lo tanto, tenemos índice de Gini del nodo: $0.5$. El árbol después del primer corte tiene\n","\n","$$\n","\\frac{50}{150} \\text{Gini izquierda} + \\frac{100}{150} \\text{Gini derecha} = 0 + \\frac{2}{3} \\frac{1}{2} = \\frac{1}{3}\\;\\;,\n","$$\n","que es menor que el $2/3$ del nodo raíz. Esta división es la que mayor **reducción de Gini** presenta.\n","\n","\n","2. **Entropía**. Se usa indistintamente, y también cumple con la propiedad de que en un nodo que contiene muestras de una sola clase, el valor de la entropía es cero.\n","\n","$$\n","- \\sum_{k=1}^K p_k \\log p_k\\;\\;.\n","$$\n","\n","En los **árboles de decisión** los cortes se realizan según *features* individuales (es decir, son cortes perpendiculares a algún eje), de manera que se minimice la \"impureza\" del árbol resultante. El algoritmo CART es *codicioso* (*greedy*), en el sentido de que elije el mejor corte (es decir, el mejor *feature*) en ese momento, sin miramientos hacia los futuros cortes. Existe la posibilidad de que un corte sub-óptimo en un nivel conduzca a un mejor resultado final, pero encontrar el árbol óptimo es un problema computacional muy complejo.\n","\n","Por lo tanto, nos contentamos con este algoritmo **greedy**.\n"]},{"cell_type":"markdown","id":"11397cbe","metadata":{"id":"11397cbe"},"source":["### Visualizando las regiones de decisión"]},{"cell_type":"markdown","id":"musical-situation","metadata":{"id":"musical-situation"},"source":["Para graficar las regiones de decisión, generamos una cuadrícula de puntos y obtenemos la predicción del modelo sobre ella.\n","\n","Luego utilizamos el método `contourf` para pintar la región. Todo esto se implementa en esta función de A. Gèron:"]},{"cell_type":"code","execution_count":null,"id":"comparable-ultimate","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"elapsed":1443,"status":"ok","timestamp":1691449613182,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"comparable-ultimate","outputId":"4b7e9878-4509-45e9-e60c-70245db1af9f"},"outputs":[],"source":["# Función adaptada de A. Gèron\n","from matplotlib.colors import ListedColormap\n","\n","def plot_decision_boundary(clf, X, t, axes=[0, 7.5, 0, 3], is_iris=True,\n","                           legend=False, plot_training=True, ngridpoints=200,\n","                           alpha=1.0, ax=None):\n","\n","    # Prepara los arreglos para colorear\n","    x1s = np.linspace(axes[0], axes[1], ngridpoints)\n","    x2s = np.linspace(axes[2], axes[3], ngridpoints)\n","\n","    # los convierte en una grilla\n","    x1, x2 = np.meshgrid(x1s, x2s)\n","\n","    # Calcula las predicciones sobre la grilla\n","    X_new = np.c_[x1.ravel(), x2.ravel()]\n","    y_pred = clf.predict(X_new).reshape(x1.shape)\n","\n","    if ax is None:\n","        ax = plt.figure().add_subplot(111)\n","\n","    # Grafica con colores esa grilla\n","    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n","    ax.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n","\n","    if not is_iris:\n","        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n","        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n","    if plot_training:\n","        sct = ax.scatter(X[:,0], X[:, 1], c=t, edgecolors='k', s=9**2,\n","                  cmap=plt.cm.rainbow, alpha=alpha)\n","        \n","        # plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n","        # plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n","        # plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris virginica\")\n","        # plt.axis(axes)\n","    if is_iris:\n","        legend1 = ax.legend(sct.legend_elements()[0], iris.target_names,\n","                            loc=\"upper left\", title=\"Clases\")\n","        ax.add_artist(legend1)\n","            \n","        ax.set_xlabel(\"Largo del pétalo\", fontsize=16)\n","        ax.set_ylabel(\"Ancho del pétalo\", fontsize=16)\n","    else:\n","        ax.set_xlabel(r\"$x_1$\", fontsize=18)\n","        ax.set_ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n","    if legend:\n","        ax.legend(loc=\"lower right\", fontsize=14)\n","\n","plt.figure(figsize=(12, 8))\n","plot_decision_boundary(tree_clf, datos, etiquetas, alpha=0.5)\n","plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n","plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n","plt.text(1.30, 1.0, \"Depth=0\", fontsize=15)\n","plt.text(3.2, 1.80, \"Depth=1\", fontsize=15)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"55792309","metadata":{"id":"55792309"},"source":["### Importancias de las características"]},{"cell_type":"markdown","id":"dbab869b","metadata":{"id":"dbab869b"},"source":["Comparando las reducciones de impurezas dadas por el mejor corte de cada característica podemos estimar la _importancias de las características_ (*feature importances*), a la que se accede a través del atributo `.feature_importances_`."]},{"cell_type":"code","execution_count":null,"id":"9d667c37","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1691448028271,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"9d667c37","outputId":"0a6babff-1ae4-44e6-f5c5-e0ca2442c53f"},"outputs":[],"source":["tree_clf.feature_importances_"]},{"cell_type":"code","execution_count":null,"id":"40cf0f79","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":830,"status":"ok","timestamp":1691448053619,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"40cf0f79","outputId":"baca7b7d-9508-4b0a-aabb-a54da6e8a19a"},"outputs":[],"source":["plt.bar(iris.feature_names[2:], tree_clf.feature_importances_)"]},{"cell_type":"markdown","id":"f3cb5761","metadata":{"id":"f3cb5761"},"source":["En este caso, vemos que ambas características paracen tener una importancia similar, con una ligera ventaja para el largo del pétalo."]},{"cell_type":"markdown","id":"banned-cemetery","metadata":{"id":"banned-cemetery"},"source":["## Sobreajuste"]},{"cell_type":"markdown","id":"fallen-pierce","metadata":{"id":"fallen-pierce"},"source":["Por defecto, un `DecisionTreeClassifier` no está regularizado, y el árbol crecerá hasta alcanzar una separación de clases perfecta.\n","\n","En el ejercicio anterior, fijamos `max_depth=2`, lo que limita el árbol a tener, a lo sumo, dos niveles de decisión. Veamos qué ocurre si sacamos esto."]},{"cell_type":"code","execution_count":null,"id":"6625494c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1691448640443,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"6625494c","outputId":"b7ad9831-14f2-4c3e-aba2-bf437e98ce57"},"outputs":[],"source":["tree_clf_noregu = DecisionTreeClassifier()\n","tree_clf_noregu.fit(datos, etiquetas)"]},{"cell_type":"markdown","id":"665c56e3","metadata":{"id":"665c56e3"},"source":["Grafiquemos el árbol resultante."]},{"cell_type":"code","execution_count":null,"id":"f400f5a5","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":653},"executionInfo":{"elapsed":1547,"status":"ok","timestamp":1691448643483,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"f400f5a5","outputId":"7a294378-bb83-4a92-e940-91b829ee9ef8"},"outputs":[],"source":["\n","plt.figure(figsize=(10, 8))\n","\n","plot_tree(tree_clf_noregu,\n","          feature_names=iris.feature_names[2:],\n","          class_names=iris.target_names,\n","          rounded=True,\n","          filled=True)\n","plt.show()"]},{"cell_type":"markdown","id":"014de972","metadata":{"id":"014de972"},"source":["Podemos averiguar cuántos cortes hizo, cuántos nodos hoja tiene y otras cosas."]},{"cell_type":"code","execution_count":null,"id":"3a54ab05","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1691448652478,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"3a54ab05","outputId":"58831c5e-e62a-48a8-9fd7-7a00a9b68cb1"},"outputs":[],"source":["print('Profundidad del árbol:', tree_clf_noregu.get_depth())\n","print('Número de nodos terminales (hoja): ', tree_clf_noregu.get_n_leaves())"]},{"cell_type":"markdown","id":"dcf0e42f","metadata":{"id":"dcf0e42f"},"source":["También podemos ver cómo cabmian las regiones de decisión"]},{"cell_type":"code","execution_count":null,"id":"f1d78f7b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1691449583451,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"f1d78f7b","outputId":"7a15894b-72bb-4ed2-a5f7-62aecac46ae5"},"outputs":[],"source":["plt.figure(figsize=(12, 8))\n","plot_decision_boundary(tree_clf_noregu, datos, etiquetas)\n","plt.show()"]},{"cell_type":"markdown","id":"3444709e","metadata":{"id":"3444709e"},"source":["La elección del mejor valor para este **hiperparámetro** debe hacerse con validación cruzada."]},{"cell_type":"code","execution_count":null,"id":"1a7a1692","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":116},"executionInfo":{"elapsed":548,"status":"ok","timestamp":1691448856864,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"1a7a1692","outputId":"7d0be742-da8a-4964-a170-24c733253769"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# Creo la búsqueda\n","treecv = GridSearchCV(estimator=tree_clf_noregu,\n","                      param_grid={'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 12]},\n","                      cv=5,\n","                      scoring='accuracy')\n","\n","# Ajusto\n","treecv.fit(datos, etiquetas)"]},{"cell_type":"markdown","id":"5768d716","metadata":{"id":"5768d716"},"source":["Como solíamos hacer, podemos acceder a un diccionario con los valores de los mejores parámetros, y también al mejor árbol, reentrenado sobre todo el conjunto de entrenamiento."]},{"cell_type":"code","execution_count":null,"id":"13108271","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"executionInfo":{"elapsed":1070,"status":"ok","timestamp":1691448861799,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"13108271","outputId":"14ed16ee-48f8-4873-e035-0fc6986bac7f"},"outputs":[],"source":["print(treecv.best_params_)\n","mejor_arbol = treecv.best_estimator_\n","\n","plt.figure(figsize=(12, 8))\n","plot_decision_boundary(mejor_arbol, datos, etiquetas)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"91f4e617","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"elapsed":976,"status":"ok","timestamp":1691448870280,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"91f4e617","outputId":"9cf98c4c-4195-4f85-bbe9-7394db23c800"},"outputs":[],"source":["plt.figure(figsize=(5, 4))\n","\n","plot_tree(mejor_arbol,\n","          feature_names=iris.feature_names[2:],\n","          class_names=iris.target_names,\n","          rounded=True,\n","          filled=True)\n","plt.show()"]},{"cell_type":"markdown","id":"0d818423","metadata":{"id":"0d818423"},"source":["## Hiperparámetros"]},{"cell_type":"markdown","id":"964aa2c7","metadata":{"id":"964aa2c7"},"source":["Además de `max_depth`, los árboles de decisión tienen un gran cantidad de hiperparámetros que se pueden ajustar para mejorar la generalización."]},{"cell_type":"markdown","id":"47140b23","metadata":{"id":"47140b23"},"source":["[texto del vínculo](https://)\n","* `max_depth`. La profundidad máxima del árbol.\n","* `min_samples_split`. El número mínimo de muestras necesarias para dividir un nodo interno. Si el número de muestras es menor que este parámetro, el nodo interno se convierte en una hoja.\n","* `min_samples_leaf`. Número mínimo de muestras (o fracción de muestra si se proporciona un punto flotante) que debe haber en un nodo hoja. Un punto de división en cualquier profundidad sólo se considerará si deja al menos ``min_samples_leaf`` muestras de entrenamiento en cada una de las ramas izquierda y derecha.\n","* `max_leaf_nodes`. Una vez crecido el árbol, sólo se conservan los mejores `max_leaf_nodes` nodos hoja. Los mejores nodos se definen a partir de la reducción relativa de la impureza.    \n","* `min_impurity_decrease`. Un nodo será dividido solo si esta división induce una disminución de la impureza mayor o igual a este valor.\n","\n","El valor óptimo de estos parámetros debe encontrarse mediante.... (todos a una) **validación cruzada**."]},{"cell_type":"code","execution_count":null,"id":"sEX27aq6OExI","metadata":{"executionInfo":{"elapsed":303,"status":"ok","timestamp":1691449062138,"user":{"displayName":"Rodrigo Fernando Diaz","userId":"14124526113481276144"},"user_tz":180},"id":"sEX27aq6OExI"},"outputs":[],"source":["DecisionTreeClassifier?"]},{"cell_type":"markdown","id":"frank-onion","metadata":{"id":"frank-onion"},"source":["## *Cortes cartesianos"]},{"cell_type":"markdown","id":"cloudy-minister","metadata":{"id":"cloudy-minister"},"source":["En principio, un árbol puede aproximar cualquier forma que tenga la frontera de decisión, pero como las divisiones binarias se realizan sobre una sola característica a la vez, la complejidad de la resultante es muy suceptible a las orientaciones de las características.\n","\n","Pongamos un ejemplo ilustrativo:"]},{"cell_type":"code","execution_count":null,"id":"distinct-intersection","metadata":{"id":"distinct-intersection"},"outputs":[],"source":["# Primera clase: 50 puntos que siguen una distribución binormal\n","size1 = 50\n","mu1 = [0, 0]\n","cov1 = [[1, 0],[0, 1]]\n","\n","# Segunda clase: 50 puntos que siguen una distribución binormal\n","size2 = 50\n","mu2 = [4, 0]\n","cov2 = [[1, 0],[0, 1]]\n","\n","# Fijamos la semilla\n","np.random.seed(42)\n","\n","# Muestreamos las clases\n","xc1 = np.random.multivariate_normal(mean=mu1, cov=cov1, size=size1).T\n","xc2 = np.random.multivariate_normal(mean=mu2, cov=cov2, size=size2).T\n","\n","# Les ponemos targets: 1 and 0\n","tc1 = np.ones((1, xc1.shape[1]))\n","tc2 = np.zeros((1, xc2.shape[1]))\n","# tc2 = -np.ones((1, xc2.shape[1]))\n","\n","# Y los juntamos en un único vector\n","x = np.hstack([xc1, xc2]).T\n","t = np.hstack([tc1, tc2]).ravel()"]},{"cell_type":"code","execution_count":null,"id":"1855bbf9","metadata":{"id":"1855bbf9"},"outputs":[],"source":["# Hacemos una gráfica\n","plt.scatter(x[t==0][:,0], x[t==0][:,1], label='class 1')\n","plt.scatter(x[t==1][:,0], x[t==1][:,1], label='class 2')\n","\n","plt.xlim(-4,8)\n","plt.ylim(-4,4)\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"circular-circumstances","metadata":{"id":"circular-circumstances"},"source":["Este conjunto de datos es linealmente separable, y los clusters se separan paralelamente a una de las características. Vamos a entrenar un DecisionTreeClassifier en este conjunto:"]},{"cell_type":"code","execution_count":null,"id":"remarkable-combat","metadata":{"id":"remarkable-combat"},"outputs":[],"source":["tree_clf = DecisionTreeClassifier()\n","tree_clf.fit(x,t)\n","\n","plot_tree(tree_clf, filled=True, rounded=True)\n","plt.show()"]},{"cell_type":"markdown","id":"round-samba","metadata":{"id":"round-samba"},"source":["El árbol resultante es extremadamente simple, así como la frontera de decisión:"]},{"cell_type":"code","execution_count":null,"id":"short-knight","metadata":{"id":"short-knight"},"outputs":[],"source":["plot_decision_boundary(tree_clf,x,t, axes=[-4, 8, -4, 4])"]},{"cell_type":"markdown","id":"younger-disposition","metadata":{"id":"younger-disposition"},"source":["Ahora rotemos los puntos 45 grados."]},{"cell_type":"code","execution_count":null,"id":"multiple-pioneer","metadata":{"id":"multiple-pioneer"},"outputs":[],"source":["x_rotated = np.zeros(x.shape)\n","x_rotated[:,0] = x[:,0]+x[:,1]\n","x_rotated[:,1] = x[:,0]-x[:,1]\n","\n","plt.scatter(x_rotated[t==0][:,0], x_rotated[t==0][:,1], label='class 1')\n","plt.scatter(x_rotated[t==1][:,0], x_rotated[t==1][:,1], label='class 2')\n","\n","plt.xlim(-4,8)\n","plt.ylim(-4,8)\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"brazilian-vancouver","metadata":{"id":"brazilian-vancouver"},"source":["El conjunto de datos sigue siendo linealmente separable, pero el límite de decisión ya no está alineado con nuestras características.\n","\n","Veamos el árbol de decisión resultante:"]},{"cell_type":"code","execution_count":null,"id":"transparent-correspondence","metadata":{"id":"transparent-correspondence"},"outputs":[],"source":["rotated_tree_clf = DecisionTreeClassifier()\n","rotated_tree_clf.fit(x_rotated,t)\n","\n","plt.figure(figsize=(5,8))\n","plot_tree(rotated_tree_clf, filled=True, rounded=True)\n","plt.show()"]},{"cell_type":"markdown","id":"discrete-chassis","metadata":{"id":"discrete-chassis"},"source":["Vemos que es mucho más complejo, pero es lo que se necesita para aproximar la nueva frontera de decisión."]},{"cell_type":"code","execution_count":null,"id":"extraordinary-battlefield","metadata":{"id":"extraordinary-battlefield"},"outputs":[],"source":["plot_decision_boundary(rotated_tree_clf,x_rotated,t, axes=[-4, 8, -4, 8])"]},{"cell_type":"markdown","id":"specified-mistake","metadata":{"id":"specified-mistake"},"source":["**Como podemos ver, es mucho más difícil para el Árbol de Decisión aprender regiones que no están alineadas con las características.**"]},{"cell_type":"markdown","id":"4f6b6265","metadata":{"id":"4f6b6265"},"source":["---"]},{"cell_type":"markdown","id":"seventh-modification","metadata":{"id":"seventh-modification","tags":[]},"source":["## Regresión"]},{"cell_type":"markdown","id":"comprehensive-wallace","metadata":{"id":"comprehensive-wallace"},"source":["Los árboles de decisión también pueden utilizarse para tareas de regresión. La idea es la misma:\n","\n","* Dividir el espacio de características en regiones definidas por umbrales en las variables.\n","* Las predicciones se calculan como la media de los objetivos en ese nodo.\n","* Las divisiones se eligen para maximizar la disminución de la función de pérdida MSE.\n","\n","Vamos a mostrar con un ejemplo de la documentación de `sklearn`"]},{"cell_type":"code","execution_count":null,"id":"secondary-spine","metadata":{"id":"secondary-spine"},"outputs":[],"source":["# Create a random dataset\n","rng = np.random.RandomState(1)\n","X = np.sort(5 * rng.rand(80, 1), axis=0)\n","y = np.sin(X).ravel()\n","y[::5] += 3 * (0.5 - rng.rand(16))\n","\n","plt.scatter(X,y)\n","plt.xlabel('$x_1$', fontsize=16)\n","plt.ylabel('$x_2$', fontsize=16)"]},{"cell_type":"markdown","id":"failing-lexington","metadata":{"id":"failing-lexington"},"source":["Ajustemos dos árboles de decisión de profundidad máxima diferente"]},{"cell_type":"code","execution_count":null,"id":"christian-overall","metadata":{"id":"christian-overall"},"outputs":[],"source":["from sklearn.tree import DecisionTreeRegressor\n","# Fit regression model\n","tree_reg_1 = DecisionTreeRegressor(max_depth=2)\n","tree_reg_2 = DecisionTreeRegressor(max_depth=5)\n","\n","tree_reg_1.fit(X,y)\n","tree_reg_2.fit(X,y)\n","\n","plt.figure(figsize=(8,5))\n","plot_tree(tree_reg_1, filled=True, rounded=True)\n","plt.show()\n","plt.figure(figsize=(20,10))\n","plot_tree(tree_reg_2, filled=True, rounded=True)\n","plt.show()"]},{"cell_type":"markdown","id":"authorized-document","metadata":{"id":"authorized-document"},"source":["¡Qué complejidad del árbol más profundo! En cada nodo, el atributo `value` nos da la predicción cada nodo."]},{"cell_type":"code","execution_count":null,"id":"objective-bikini","metadata":{"id":"objective-bikini"},"outputs":[],"source":["# Predict\n","x_ = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n","y_1 = tree_reg_1.predict(x_)\n","y_2 = tree_reg_2.predict(x_)\n","\n","# Plot the results\n","plt.figure(figsize=(12, 8))\n","plt.scatter(X, y, s=20, edgecolor=\"black\",\n","            c=\"darkorange\", label=\"data\")\n","plt.plot(x_, y_1, color=\"cornflowerblue\",\n","         label=\"max_depth=2\", linewidth=2)\n","plt.plot(x_, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n","plt.xlabel(\"X\", fontsize=16)\n","plt.ylabel(\"Target\", fontsize=16)\n","plt.title(\"Decision Tree Regression\", fontsize=18)\n","plt.legend(fontsize=16)\n","plt.show()"]},{"cell_type":"markdown","id":"9e62a8a2","metadata":{"id":"9e62a8a2"},"source":["## Árboles aleatorios"]},{"cell_type":"markdown","id":"f6fc05cb","metadata":{"id":"f6fc05cb"},"source":["Incluso con todos los valores de los hiperparámetros fijados, el árbol resultante no está definido de forma determinista. Es decir que dos corridas seguidas pueden producir árboles (ligeramente) diferentes. Esto es parecido a lo que pasaba con el **perceptrón** al comenzar desde valores de los pesos diferentes.\n","\n","Pero acá el origen de la diferencia es otra. En cada división se realiza una barajada aleatoria de las características. Si hay dos divisiones que producen la misma reducción de impureza, se utilizará la primera y este orden relativo podría cambiar entre las ejecuciones. Por lo tanto, es posible que queramos establecer el `random_state` para obtener resultados reproducibles."]},{"cell_type":"markdown","id":"f274830d","metadata":{"id":"f274830d"},"source":["También podemos aleatorizar el resultado de nuestro árbol adrede. Esto será útil más adelante cuando consideremos conjuntos de árboles (en Random Forests). Los parámetros para aleatorizar el árbol resultante son\n","\n","* `splitter`: Para cada característica, el algoritmo encuentra la mejor división y calcula su importancia (la reducción de impurezas), dada por el parámetro `criterion` (que puede ser `gini` o `entropy`). Si `splitter` se elije como `best`, se elige el mejor corte de la mejor característica. Si `splitter='random'`, se eligen cortes al azar para cada característica y se utiliza el mejor entre ellos.\n","\n","* `max_features`: En cada división, se considera sólo un subconjunto aleatorio de características `max_features`. Las opciones son:\n","\n","  * un int. Se consideran `max_features` características en cada división.\n","  * un float. `max_features` es una fracción y se consideran max(1, int(max_features * n_features_in_)) en cada división.\n","  * \"auto\". `max_features=sqrt(n_features)`.\n","  * \"sqrt\". `max_features=sqrt(n_features)`.\n","  * \"log2\". `max_features=log2(n_features)`.\n","  * \"None\". entonces `max_features=n_features`."]},{"cell_type":"markdown","id":"b6f43fee","metadata":{},"source":["Probemos esto con un dataset que van a usar para el ejercicio de la semana."]},{"cell_type":"code","execution_count":null,"id":"6780c565","metadata":{},"outputs":[],"source":["from sklearn.datasets import make_moons\n","X, t = make_moons(n_samples=200, noise=0.15, random_state=42)\n","\n","plt.scatter(X[:,0], X[:,1], c=t+5, cmap='rainbow')"]},{"cell_type":"code","execution_count":null,"id":"7002d76a","metadata":{},"outputs":[],"source":["tree_clf_luna = DecisionTreeClassifier(max_depth=3)\n"]},{"cell_type":"code","execution_count":null,"id":"6ab148eb","metadata":{},"outputs":[],"source":["\n","tree_clf_luna.fit(X, t)\n","plot_decision_boundary(tree_clf_luna, X, t, \n","                        is_iris=False, axes=[-1.7, 2.2, -1.2, 2.2])"]},{"cell_type":"code","execution_count":null,"id":"3ac19cf0","metadata":{"id":"3ac19cf0"},"outputs":[],"source":["tree_clf_random = DecisionTreeClassifier(max_depth=4, splitter='best', max_features=1)"]},{"cell_type":"code","execution_count":null,"id":"d719fad8","metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(10, 10))\n","\n","for i in range(9):\n","    tree_clf_random.fit(X, t)\n","    ax = fig.add_subplot(3, 3, i+1)\n","    plot_decision_boundary(tree_clf_random, X, t, \n","                           is_iris=False, axes=[-1.7, 2.5, -1.2, 2.], ax=ax, legend=False)"]},{"cell_type":"markdown","id":"b49ca507","metadata":{},"source":["### Extra aleatorios"]},{"cell_type":"code","execution_count":null,"id":"5754cc48","metadata":{},"outputs":[],"source":["tree_clf_random = DecisionTreeClassifier(max_depth=3, splitter='random', max_features=1)"]},{"cell_type":"code","execution_count":null,"id":"7822c5b7","metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(10, 10))\n","\n","for i in range(9):\n","    tree_clf_random.fit(X, t)\n","    ax = fig.add_subplot(3, 3, i+1)\n","    plot_decision_boundary(tree_clf_random, X, t, \n","                           is_iris=False, axes=[-1.7, 2.5, -1.2, 2.], ax=ax, legend=False)"]},{"cell_type":"code","execution_count":null,"id":"8b780006","metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["frank-onion","seventh-modification","9e62a8a2"],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('tf-mac')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"2b7aa682480b82eb27ca7b5ecfebdb0027bb2a276e6bdff64c1ddeab03557e9e"}}},"nbformat":4,"nbformat_minor":5}
